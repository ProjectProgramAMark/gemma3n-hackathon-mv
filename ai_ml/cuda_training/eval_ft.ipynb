{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating fine tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "# from unsloth import FastLanguageModel\n",
    "# leaving Trainer out for now to use SFTTrainer instead\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "import os\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggles\n",
    "\n",
    "chat = False\n",
    "# chat = True\n",
    "# prompt_format = \"llama\"\n",
    "prompt_format = \"mistral\"\n",
    "anton = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: restart kernel so that we don't run out of memory re-loading base model\n",
    "\n",
    "# Loading base model again\n",
    "# model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "# model_id = \"filipealmeida/Mistral-7B-v0.1-sharded\"\n",
    "# model_id = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n",
    "# model_id = \"imiraoui/OpenHermes-2.5-Mistral-7B-sharded\"\n",
    "# model_id = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"google/gemma-7b-it\"\n",
    "# model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type=\"nf4\")\n",
    "# bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map='auto')\n",
    "\n",
    "# for anton adapters, need to resize model and token embeddings for added token special vocabularly\n",
    "if anton:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, add_bos_token=True)\n",
    "    DEFAULT_PAD_TOKEN = \"<|pad|>\"\n",
    "    DEFAULT_EOS_TOKEN = \"<|endoftext|>\"\n",
    "    DEFAULT_UNK_TOKEN = \"<|unk|>\"\n",
    "\n",
    "    special_tokens_dict = dict()\n",
    "    if tokenizer.pad_token is None:\n",
    "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "    if tokenizer.unk_token is None:\n",
    "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if prompt_format == \"llama\":\n",
    "    llama = True\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, add_bos_token=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        DEFAULT_PAD_TOKEN = \"<|pad|>\"\n",
    "        DEFAULT_UNK_TOKEN = \"<|unk|>\"\n",
    "        special_tokens_dict = dict()\n",
    "        if tokenizer.pad_token is None:\n",
    "            special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "        if tokenizer.unk_token is None:\n",
    "            special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "        tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Merging base model with fine-tuned LoRa adapter\n",
    "# WORKS\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/ft-mistral-7b-instruct-v02-02-11-24-anton\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/ft-mistral-7b-instruct-v02-final-anton\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/ft-mistral-7b-instruct-v02-03-02-24-4-breaking-test-2\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/ft-gemma-7b-it-03-01-24/\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/ft-mistral-7b-instruct-v02-03-02-24-4-breaking-test-3-50-epochs\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/llama-3-8b-instruct-1\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/llama-3-8b-instruct-2\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/llama-3-8b-instruct-3\")\n",
    "# TESTING\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/gemma-2b-it-03-02-24\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/kto-ft-mistral-7b-instruct-v02-1\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/kto-ft-mistral-7b-instruct-v02-2\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/kto-ft-mistral-7b-instruct-v02-3\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/llama-3-8b-instruct-KTO-1\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/llama-3-8b-instruct-KTO-2\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/ft-mistral-7b-instruct-v03-04\")\n",
    "ft_model = PeftModel.from_pretrained(base_model, './models/ft-mistral-7b-instruct-v03-10')\n",
    "# GOOD MODEL TO GO WITH\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/ft-mistral-7b-instruct-v02-03-02-24-3\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/gemma-2b-it-03-03-24-2\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/ft-mistral-7b-instruct-v02-final\")\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"./models/llama-3-8b-instruct-4\")\n",
    "ft_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating for ChatML formatted prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chat:\n",
    "    if prompt_format == \"mistral\":\n",
    "        # for mistral\n",
    "        ft_model_input = [\n",
    "            {\"role\": \"user\", \"content\": \"\"\"Assist a non-verbal autistic individual in communicating their thoughts or needs through selected words.\n",
    "             Your task: infer and articulate the message in first-person, i.e. using I and pretending you are the user.\n",
    "             - Be concise and only provide the answer to the following input.\n",
    "             - Look for deeper meanings in the input.\n",
    "             - Keep the tone practical and straightforward.\n",
    "                input: fins, mask, snorkel, scuba\"\"\",\n",
    "            },\n",
    "            { \"role\": \"assistant\", \"content\": \"\"}\n",
    "            # { \"role\": \"model\", \"content\": \"\"}\n",
    "        ]\n",
    "    elif prompt_format == \"llama\":\n",
    "        # for llama\n",
    "        ft_model_input = [\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"\n",
    "             Assist a non-verbal autistic individual in communicating their thoughts or needs through selected images.\n",
    "             Your task: infer and articulate the message in first-person, i.e. using I and pretending you are the user.\n",
    "             - Be concise and only provide the answer to the following input.\n",
    "             - Be empathetic and direct.\n",
    "             - Look for deeper meanings in the input.\n",
    "             - Keep the tone practical and straightforward.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": \"fins, mask, snorkel\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ]\n",
    "\n",
    "    # Re-init the tokenizer so it doesn't add padding or eos token\n",
    "    if prompt_format == \"mistral\":\n",
    "        # for mistral\n",
    "        ft_tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, padding_side=\"left\")\n",
    "    else:\n",
    "        ft_tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, padding_side=\"right\")\n",
    "\n",
    "    # DEFAULT_PAD_TOKEN = \"<|pad|>\"\n",
    "    # DEFAULT_EOS_TOKEN = \"<|endoftext|>\"\n",
    "    # DEFAULT_UNK_TOKEN = \"<|unk|>\"\n",
    "\n",
    "    if ft_tokenizer.pad_token is None:\n",
    "        ft_tokenizer.pad_token = ft_tokenizer.unk_token\n",
    "        # ft_tokenizer.pad_token = DEFAULT_PAD_TOKEN\n",
    "        # ft_tokenizer.eos_token = DEFAULT_EOS_TOKEN\n",
    "\n",
    "    ft_model_input_tokenized = ft_tokenizer.apply_chat_template(ft_model_input, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # print(f'ft_model_input_tokenized: {ft_model_input_tokenized}')\n",
    "\n",
    "    # ft_model.eval()\n",
    "\n",
    "    generated_output = ft_model.generate(\n",
    "        ft_model_input_tokenized,\n",
    "        max_new_tokens=500,\n",
    "        pad_token_id=ft_tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    ft_model_output = ft_tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    print(ft_model_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating for non-ChatML formatted prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not chat:    \n",
    "    ft_model_input = f\"\"\"[INST] Assist a non-verbal autistic individual in communicating their thoughts or needs through selected text input.\n",
    "    Your task: infer and articulate the message in first-person, using simple, direct language with empathy and clarity. Be concise. \n",
    "    Only give the output for the input provided. Do not come up with new inputs after. Assume the user is trying to communicate with someone.\n",
    "    Usually those are wants, desires, needs, etc.\n",
    "\n",
    "    ### Input: scuba, fins, mask, snorkel, store [/INST]\n",
    "    ### Output:\"\"\"\n",
    "\n",
    "    # Re-init the tokenizer so it doesn't add padding or eos token\n",
    "    ft_tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, padding_side=\"left\")\n",
    "\n",
    "    DEFAULT_PAD_TOKEN = \"<|pad|>\"\n",
    "    DEFAULT_EOS_TOKEN = \"<|endoftext|>\"\n",
    "    DEFAULT_UNK_TOKEN = \"<|unk|>\"\n",
    "\n",
    "    if ft_tokenizer.pad_token is None:\n",
    "        ft_tokenizer.pad_token = ft_tokenizer.unk_token\n",
    "        # ft_tokenizer.unk_token = DEFAULT_UNK_TOKEN\n",
    "        # ft_tokenizer.pad_token = DEFAULT_PAD_TOKEN\n",
    "        # ft_tokenizer.eos_token = DEFAULT_EOS_TOKEN\n",
    "\n",
    "    # debugging\n",
    "    # print(f'tokenizer.pad_token: {ft_tokenizer.pad_token}')\n",
    "    # print(f'tokenizer.eos_token: {ft_tokenizer.eos_token}')\n",
    "    # print(f'tokenizer.unk_token: {ft_tokenizer.unk_token}')\n",
    "\n",
    "    ft_model_input_tokenized = ft_tokenizer(ft_model_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    ft_model.eval()\n",
    "\n",
    "    # Generate model output with specified parameters\n",
    "    generated_output = ft_model.generate(\n",
    "        **ft_model_input_tokenized,\n",
    "        max_new_tokens=100,\n",
    "        pad_token_id=ft_tokenizer.unk_token_id,\n",
    "        do_sample=False,\n",
    "        output_scores=True\n",
    "    )\n",
    "    ft_model_output = ft_tokenizer.decode(generated_output[0], skip_special_tokens=False)\n",
    "    print(ft_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Base Model to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "# # from unsloth import FastLanguageModel\n",
    "# # leaving Trainer out for now to use SFTTrainer instead\n",
    "# from trl import SFTTrainer\n",
    "# from datasets import load_dataset, Dataset, DatasetDict\n",
    "# from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "# import os\n",
    "# import torch\n",
    "\n",
    "# model_type = \"mistral\"\n",
    "\n",
    "# if model_type == \"chat_ml\":\n",
    "#     model_id = \"google/gemma-2b-it\"\n",
    "# else:\n",
    "#     model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_type=torch.float16, bnb_4bit_quant_type=\"nf4\")\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map='auto')\n",
    "# if model_type == \"chat_ml\":\n",
    "#     base_tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, add_bos_token=True)\n",
    "#     base_model_input = [\n",
    "#         {\n",
    "#             \"role\": \"user\", \"content\": \"\"\"Assist a non-verbal autistic individual in communicating their thoughts or needs through selected images.\n",
    "#             Your task: infer and articulate the message in first-person, using simple, direct language with empathy and clarity.\n",
    "#             - Be empathetic and direct.\n",
    "#             - Look for deeper meanings in the input.\n",
    "#             - Keep the tone practical and straightforward.\n",
    "#             Only give the output for the input provided. Do not come up with new inputs abaseer.\n",
    "#             input: fins, mask, snorkel\"\"\",\n",
    "#         },\n",
    "#         # { \"role\": \"assistant\", \"content\": \"\"}\n",
    "#         { \"role\": \"model\", \"content\": \"\"}\n",
    "#     ]\n",
    "#     base_model_input_tokenized = base_tokenizer.apply_chat_template(base_model_input, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "#     generated_output = base_model.generate(\n",
    "#         base_model_input_tokenized,\n",
    "#         max_new_tokens=200,\n",
    "#         pad_token_id=base_tokenizer.pad_token_id\n",
    "#     )\n",
    "# else:\n",
    "#     base_tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, padding_side=\"left\")\n",
    "#     if base_tokenizer.pad_token is None:\n",
    "#         base_tokenizer.pad_token = base_tokenizer.unk_token\n",
    "#     base_model_input = f\"\"\"[INST] Assist a non-verbal autistic individual in communicating their thoughts or needs through selected text input.\n",
    "#     Your task: infer and articulate the message in first-person, using simple, direct language with empathy and clarity. Be concise. \n",
    "#     Only give the output for the input provided. Do not come up with new inputs after. \n",
    "\n",
    "#     ### Input: Porter Robinson, Worlds, Nurture, Look at the Sky [/INST]\n",
    "#     ### Output:\"\"\"\n",
    "#     base_model_input_tokenized = base_tokenizer(base_model_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "#     # print(f'base_model_input_tokenized: {base_model_input_tokenized}')\n",
    "#     # base_model.eval()\n",
    "#     generated_output = base_model.generate(\n",
    "#         **base_model_input_tokenized,\n",
    "#         max_new_tokens=200,\n",
    "#         pad_token_id=base_tokenizer.pad_token_id\n",
    "#     )\n",
    "\n",
    "# base_model_output = base_tokenizer.decode(generated_output[0], skip_special_tokens=False)\n",
    "# print(base_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the functions from eval_ft.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_ft import load_model, generate_tokenizer, merge_model, format_prompt, generate_output, read_user_inputs, eval_base_model\n",
    "import random\n",
    "\n",
    "unsloth = True\n",
    "anton = False\n",
    "\n",
    "# gemma 2b\n",
    "# model_id = \"unsloth/gemma-2b-it-bnb-4bit\"\n",
    "# model_path = \"./models/gemma-2b-it-06-13-24-unsloth-1\"\n",
    "# prompt_format = \"gemma\"\n",
    "# chat = True\n",
    "\n",
    "# phi 3\n",
    "# model_id = \"unsloth/phi-3-instruct-v0.3-bnb-4bit\"\n",
    "# model_path = \"./models/phi-3-06-29-24-unsloth-1\"\n",
    "# prompt_format = \"chat_ml\"\n",
    "# chat = True\n",
    "\n",
    "# mistral\n",
    "model_id = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
    "model_path = './models/ft-mistral-7b-instruct-v03-10'\n",
    "prompt_format = \"mistral\"\n",
    "chat = False\n",
    "\n",
    "input_file_path = None\n",
    "# input_file_path = \"./data/processed_dataset_full.jsonl\"\n",
    "num_inputs = 1\n",
    "user_input = \"fish, camp, pee\"\n",
    "\n",
    "\n",
    "base_model = load_model(model_id, unsloth)\n",
    "ft_tokenizer = generate_tokenizer(model_id, base_model, anton, prompt_format, unsloth)\n",
    "ft_model = merge_model(base_model, model_path, unsloth)\n",
    "if input_file_path:\n",
    "    user_inputs = read_user_inputs(input_file_path)\n",
    "    for i in range(num_inputs):\n",
    "        user_input = user_inputs[random.randint(0, len(user_inputs)-1)]\n",
    "        prompt = format_prompt(user_input, chat, prompt_format)\n",
    "        generate_output(ft_tokenizer, ft_model, prompt, chat, prompt_format)\n",
    "elif user_input:\n",
    "    prompt = format_prompt(user_input, chat, prompt_format)\n",
    "    generate_output(ft_tokenizer, ft_model, prompt, chat, prompt_format)\n",
    "else:\n",
    "    raise ValueError('No input or input file path provided to evaluate model.')\n",
    "\n",
    "# if eval_base_model:\n",
    "#     eval_base_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_fine_tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
